{\rtf1\ansi\ansicpg1252\cocoartf1348\cocoasubrtf170
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
\paperw11900\paperh16840\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural

\f0\fs24 \cf0 \
README.txt\
\
Script was adapted from fLoc\
(Stander vision and perception lab, Grill-Spector, available on gitHub)\
\
\
Localizer task adapted:\
\
1) Reduce the number of categories\
- Only need Face, Scenes, Objects and Scrambled Objects.\
\
2) Fit task structure used in Graham\'92s and Barense\'92s labs \
- Blocks of 20 Images, each image for 300ms + ISI 450ms\
- 12 blocks per run\
- 4 runs, but prompted at start of task.\

3) Fix what seemed to be a bug in record_keys function
- (or maybe it just does things differently or our devices)
- Keyboard responses were recorded as 1 if nothing recorded and 0 if something got recorded.
- Changed to the opposite. So 1 = key has beeen pressed.

4) Structure:
- 4 conditions + Baseline
- Each block contains 15 stimuli of 1 condition (eg. 15 x faces).
- Every image is displayed for 300ms followed by 450ms; 1 block = 11.25 seconds total
- One run contains 4 times each 4 conditions (16 blocks), the order being generated in an optimal fashion.
- 1 run Starts with a Baseline (fixation) condition for 11.25 seconds (same duration as conditions block).
- 1 run lasts a little more than 3 minutes (191 seconds)

}